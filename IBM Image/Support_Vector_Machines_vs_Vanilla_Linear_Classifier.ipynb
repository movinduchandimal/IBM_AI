{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine vs Vanilla Linear Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<p>We will be classifying the popular handwritten data set which we can find in the sklearn library and comparing the results of the logistic regression and SVM.  In the Sklearn library, there are several ways to use logistic regression for multiclass applications; in this lab, we will use  the `multinomial` option; this is like Softmax function we discussed before</p>\n",
    "\n",
    "<ul>\n",
    "    <li>Plotting an Image</li>\n",
    "    <li>Preprocess data for Logistic Regression</li>\n",
    "    <li>Logistic Regression with SkLearn</li>\n",
    "    <li>SVM for Image Classification with SkLearn</li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>60 min</strong></p>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Important Libraries and Digit Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, svm, metrics, model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/datasets/base.py:550: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target = data[:, -1].astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = digits.target\n",
    "flatten_digits = digits.images.reshape((len(digits.images), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Some Handwritten Images in the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=5, figsize=(10, 4))\n",
    "for ax, image, label in zip(axes, digits.images, target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('%i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Images into Training and Test Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have set the test size to 20% of the total dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(flatten_digits, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-written classification with Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the dataset to put all the features of the variables on the same scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_logistic = scaler.fit_transform(X_train)\n",
    "X_test_logistic = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the logistic regression and fit the logistic regression and use the <code>l1</code> penalty. Note here that since this is a multiclass problem the Logistic Regression parameter `multi_class` is set to `multinomial`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=0.01, penalty='l1', solver='saga', tol=0.1, multi_class='multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.fit(X_train_logistic, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logistic = logit.predict(X_test_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the accuracy of the logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: \"+str(logit.score(X_test_logistic, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot out the confusion matrix, each row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "cmx = confusion_matrix(y_test, y_pred_logistic, labels=label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is fine and above 80% but we can see some heavily misclassified values, The classifier had a hard time classifying <code>8</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(cmx)\n",
    "# plt.figure(figsize=(10,7))\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "title = \"Confusion Matrix for SVM results\"\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-Written Classification with SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and fit the SVM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = svm.SVC(gamma='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict for our test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svm_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get accuracy for the SVM model, we can see we have a nearly perfect model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: \"+str(accuracy_score(y_test, y_pred_svm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the confusion matrix for SVM, we can see a nearly perfect model with SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "cmx = confusion_matrix(y_test, y_pred_svm, labels=label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(cmx)\n",
    "# plt.figure(figsize=(10,7))\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "title = \"Confusion Matrix for SVM results\"\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing both SVM and Logistic Regression with K-Fold Cross Validation\n",
    "\n",
    "k-fold Cross validation is used when there are limited samples, the handwritten dataset contains about 1800 samples, this will give an opportunity for all the data to be in the training and test set at different given times. We will add <code>l2</code> regularization to visualize how well they both do against SVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = []\n",
    "algorithm.append(('SVM', svm_classifier))\n",
    "algorithm.append(('Logistic_L1', logit))\n",
    "algorithm.append(('Logistic_L2', LogisticRegression(C=0.01, penalty='l2', solver='saga', tol=0.1, multi_class='multinomial')))\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "y = digits.target\n",
    "for name, algo in algorithm:\n",
    "    k_fold = model_selection.KFold(n_splits=10, random_state=10)\n",
    "    if name == 'SVM':\n",
    "        X = flatten_digits\n",
    "        cv_results = model_selection.cross_val_score(algo, X, y, cv=k_fold, scoring='accuracy')\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(flatten_digits)\n",
    "        cv_results = model_selection.cross_val_score(algo, X, y, cv=k_fold, scoring='accuracy')\n",
    "        \n",
    "    results.append(cv_results)\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot and we can see that SVM performs better all the time even with k-fold cross validation and it is better than both Logistic regressions on average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle('Compare Logistic and SVM results')\n",
    "ax = fig.add_subplot()\n",
    "plt.boxplot(results)\n",
    "plt.ylabel('Accuracy')\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  [Recognizing Hand-written](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html?utm_medium=Exinfluencer&utm_source=Nurture&utm_content=000026UJ&utm_term=10006555&utm_id=SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-CV0101EN-Coursera-25797139&utm_email=Email&utm_campaign=PLACEHOLDER)\n",
    "2.  [MNIST classification using multinomial logistic + L1](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html?utm_medium=Exinfluencer&utm_source=Nurture&utm_content=000026UJ&utm_term=10006555&utm_id=SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-CV0101EN-Coursera-25797139&utm_email=Email&utm_campaign=PLACEHOLDER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Authors</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Aije Egwaikhide](https://www.linkedin.com/in/aije-egwaikhide/?utm_medium=Exinfluencer&utm_source=Nurture&utm_content=000026UJ&utm_term=10006555&utm_id=SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-CV0101EN-Coursera-25797139&utm_email=Email&utm_campaign=PLACEHOLDER) is a Data Scientist at IBM who holds a degree in Economics and Statistics from the University of Manitoba and a Post-grad in Business Analytics from St. Lawrence College, Kingston. She is currently pursuing her Masters in Management Analytics at Queens University. She is part of the IBM Developer Skills Network group where she brings her real-world experience to the courses she creates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]  <a href='https://opencv.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2022-01-01'>Open CV</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Change Log</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th>Date (YYYY-MM-DD)</th>\n",
    "        <th>Version</th>\n",
    "        <th>Changed By</th>\n",
    "        <th>Change Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2021-03-30</td>\n",
    "        <td>0.1</td>\n",
    "        <td>Aije</td>\n",
    "        <td>Created original version of the lab</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2020 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
